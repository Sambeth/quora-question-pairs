{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras import metrics, models, optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers import (\n",
    "    TimeDistributed,\n",
    "    Lambda, Convolution1D,\n",
    "    GlobalMaxPooling1D,\n",
    "    merge,\n",
    "    add,\n",
    "    SpatialDropout1D,\n",
    "    Reshape,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    RepeatVector,\n",
    "    InputLayer\n",
    ")\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.preprocessing import sequence, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_labels = data['is_duplicate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tk = text.Tokenizer(num_words=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 40\n",
    "tk.fit_on_texts(list(data['question1'].values) + list(data['question2'].values.astype(str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad the questions to all have the same length of 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1 = tk.texts_to_sequences(data['question1'].values)\n",
    "x1 = sequence.pad_sequences(x1, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x2 = tk.texts_to_sequences(data['question2'].values.astype(str))\n",
    "x2 = sequence.pad_sequences(x2, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = tk.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the word embedding using the glove text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:36, 10990.32it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('/glove/glove.6B.300d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95596/95596 [00:00<00:00, 339280.82it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Construct a very simple Long Short-Term Memory neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input1 = Input(shape=(max_len,), dtype='int32')\n",
    "model1 = Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     trainable=False\n",
    "                    )(input1)\n",
    "model1 = TimeDistributed(Dense(300, activation='relu'))(model1)\n",
    "model1 = Lambda(lambda x: K.sum(x, axis=1), output_shape=(300,))(model1)\n",
    "\n",
    "\n",
    "input2 = Input(shape=(max_len,), dtype='int32')\n",
    "model2 = Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     trainable=False\n",
    "                    )(input2)\n",
    "model2 = TimeDistributed(Dense(300, activation='relu'))(model2)\n",
    "model2 = Lambda(lambda x: K.sum(x, axis=1), output_shape=(300,))(model2)\n",
    "\n",
    "\n",
    "input3 = Input(shape=(max_len,), dtype='int32')\n",
    "model3 = Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     input_length=40,\n",
    "                    )(input3)\n",
    "model3 = Dropout(0.3)(model3)\n",
    "model3 = LSTM(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)(model3)\n",
    "model3 = Lambda(lambda x: K.sum(x, axis=1), output_shape=(300,))(model3)\n",
    "\n",
    "\n",
    "input4 = Input(shape=(max_len,), dtype='int32')\n",
    "model4 = Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     input_length=40,\n",
    "                    )(input4)\n",
    "model4 = Dropout(0.3)(model4)\n",
    "model4 = LSTM(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)(model4)\n",
    "model4 = Lambda(lambda x: K.sum(x, axis=1), output_shape=(300,))(model4)\n",
    "\n",
    "\n",
    "merged_model = Concatenate()([model1, model2, model3, model4])\n",
    "\n",
    "merged_model = BatchNormalization()(merged_model)\n",
    "\n",
    "merged_model = Dense(300)(merged_model)\n",
    "merged_model = PReLU()(merged_model)\n",
    "merged_model = Dropout(0.3)(merged_model)\n",
    "merged_model = BatchNormalization()(merged_model)\n",
    "\n",
    "merged_model = Dense(300)(merged_model)\n",
    "merged_model = PReLU()(merged_model)\n",
    "merged_model = Dropout(0.3)(merged_model)\n",
    "merged_model = BatchNormalization()(merged_model)\n",
    "\n",
    "merged_model = Dense(300)(merged_model)\n",
    "merged_model = PReLU()(merged_model)\n",
    "merged_model = Dropout(0.3)(merged_model)\n",
    "merged_model = BatchNormalization()(merged_model)\n",
    "\n",
    "merged_model = Dense(1)(merged_model)\n",
    "merged_model = Activation('sigmoid')(merged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 40, 300)      28679100    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 40, 300)      28679100    input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 40, 300)      28679100    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 40, 300)      28679100    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 40, 300)      0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 40, 300)      0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 40, 300)      90300       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 40, 300)      90300       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 40, 300)      721200      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 40, 300)      721200      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 300)          0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 300)          0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 300)          0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 300)          0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1200)         0           lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1200)         4800        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 300)          360300      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 300)          300         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 300)          0           p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 300)          1200        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 300)          90300       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_2 (PReLU)               (None, 300)          300         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 300)          0           p_re_lu_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 300)          1200        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 300)          90300       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_3 (PReLU)               (None, 300)          300         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 300)          0           p_re_lu_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 300)          1200        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            301         batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 116,889,901\n",
      "Trainable params: 59,527,501\n",
      "Non-trainable params: 57,362,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Model(inputs=[input1, input2, input3, input4], outputs=merged_model)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 363861 samples, validate on 40429 samples\n",
      "Epoch 1/15\n",
      "363648/363861 [============================>.] - ETA: 0s - loss: 0.5449 - acc: 0.7187\n",
      "Epoch 00001: val_acc improved from -inf to 0.76641, saving model to weight.h5\n",
      "363861/363861 [==============================] - 346s 950us/step - loss: 0.5449 - acc: 0.7187 - val_loss: 0.4714 - val_acc: 0.7664\n",
      "Epoch 2/15\n",
      "363648/363861 [============================>.] - ETA: 0s - loss: 0.4644 - acc: 0.7712\n",
      "Epoch 00002: val_acc improved from 0.76641 to 0.78070, saving model to weight.h5\n",
      "363861/363861 [==============================] - 343s 942us/step - loss: 0.4644 - acc: 0.7712 - val_loss: 0.4428 - val_acc: 0.7807\n",
      "Epoch 3/15\n",
      "363648/363861 [============================>.] - ETA: 0s - loss: 0.4198 - acc: 0.7988\n",
      "Epoch 00003: val_acc improved from 0.78070 to 0.79500, saving model to weight.h5\n",
      "363861/363861 [==============================] - 343s 943us/step - loss: 0.4198 - acc: 0.7988 - val_loss: 0.4213 - val_acc: 0.7950\n",
      "Epoch 4/15\n",
      "363648/363861 [============================>.] - ETA: 0s - loss: 0.3850 - acc: 0.8196\n",
      "Epoch 00004: val_acc did not improve\n",
      "363861/363861 [==============================] - 333s 915us/step - loss: 0.3851 - acc: 0.8196 - val_loss: 0.4199 - val_acc: 0.7949\n",
      "Epoch 5/15\n",
      "363648/363861 [============================>.] - ETA: 0s - loss: 0.3556 - acc: 0.8360\n",
      "Epoch 00005: val_acc improved from 0.79500 to 0.80000, saving model to weight.h5\n",
      "363861/363861 [==============================] - 343s 943us/step - loss: 0.3556 - acc: 0.8360 - val_loss: 0.4124 - val_acc: 0.8000\n",
      "Epoch 6/15\n",
      "363648/363861 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.8519\n",
      "Epoch 00006: val_acc improved from 0.80000 to 0.81071, saving model to weight.h5\n",
      "363861/363861 [==============================] - 343s 943us/step - loss: 0.3255 - acc: 0.8519 - val_loss: 0.4105 - val_acc: 0.8107\n",
      "Epoch 7/15\n",
      "363648/363861 [============================>.] - ETA: 0s - loss: 0.2980 - acc: 0.8663\n",
      "Epoch 00007: val_acc improved from 0.81071 to 0.81634, saving model to weight.h5\n",
      "363861/363861 [==============================] - 343s 943us/step - loss: 0.2980 - acc: 0.8663 - val_loss: 0.4246 - val_acc: 0.8163\n",
      "Epoch 8/15\n",
      "363648/363861 [============================>.] - ETA: 0s - loss: 0.2713 - acc: 0.8796\n",
      "Epoch 00008: val_acc improved from 0.81634 to 0.81884, saving model to weight.h5\n",
      "363861/363861 [==============================] - 343s 943us/step - loss: 0.2713 - acc: 0.8796 - val_loss: 0.4361 - val_acc: 0.8188\n",
      "Epoch 9/15\n",
      "363648/363861 [============================>.] - ETA: 0s - loss: 0.2482 - acc: 0.8913\n",
      "Epoch 00009: val_acc did not improve\n",
      "363861/363861 [==============================] - 333s 916us/step - loss: 0.2482 - acc: 0.8913 - val_loss: 0.4380 - val_acc: 0.8159\n",
      "Epoch 10/15\n",
      "363648/363861 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9004\n",
      "Epoch 00010: val_acc did not improve\n",
      "363861/363861 [==============================] - 333s 916us/step - loss: 0.2277 - acc: 0.9004 - val_loss: 0.4852 - val_acc: 0.8143\n",
      "Epoch 11/15\n",
      "363648/363861 [============================>.] - ETA: 0s - loss: 0.2104 - acc: 0.9089\n",
      "Epoch 00011: val_acc improved from 0.81884 to 0.81889, saving model to weight.h5\n",
      "363861/363861 [==============================] - 343s 943us/step - loss: 0.2104 - acc: 0.9089 - val_loss: 0.5123 - val_acc: 0.8189\n",
      "Epoch 12/15\n",
      "363648/363861 [============================>.] - ETA: 0s - loss: 0.1951 - acc: 0.9160\n",
      "Epoch 00012: val_acc did not improve\n",
      "363861/363861 [==============================] - 333s 916us/step - loss: 0.1951 - acc: 0.9160 - val_loss: 0.5244 - val_acc: 0.8179\n",
      "Epoch 13/15\n",
      "363648/363861 [============================>.] - ETA: 0s - loss: 0.1809 - acc: 0.9227\n",
      "Epoch 00013: val_acc did not improve\n",
      "363861/363861 [==============================] - 333s 916us/step - loss: 0.1809 - acc: 0.9227 - val_loss: 0.5421 - val_acc: 0.8182\n",
      "Epoch 14/15\n",
      "363648/363861 [============================>.] - ETA: 0s - loss: 0.1688 - acc: 0.9289\n",
      "Epoch 00014: val_acc improved from 0.81889 to 0.82050, saving model to weight.h5\n",
      "363861/363861 [==============================] - 334s 919us/step - loss: 0.1688 - acc: 0.9289 - val_loss: 0.5807 - val_acc: 0.8205\n",
      "Epoch 15/15\n",
      "354432/363861 [============================>.] - ETA: 8s - loss: 0.1583 - acc: 0.9335"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('weight.h5', monitor='val_acc', save_best_only=True, verbose=2)\n",
    "\n",
    "model.fit([x1, x2, x1, x2], y=y_labels, batch_size=384, nb_epoch=15,\n",
    "                 verbose=1, validation_split=0.1, shuffle=True,\n",
    "                 callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here a loss of 0.1688 and a validation loss of 0.5807. There may be some overfitting here. This model can be improved by adding a convolution layer and adjusting the dropout rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
